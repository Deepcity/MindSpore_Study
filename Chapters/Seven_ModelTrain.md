## æ¨¡å‹è®­ç»ƒ

æœ¬ç‰‡æ¶‰åŠå¤§é‡ä¹‹å‰æåˆ°æ¦‚å¿µçš„å®šä¹‰ä»¥åŠå¤§é‡å¼•ç”¨MindSporeå®˜æ–¹æ–‡æ¡£ã€‚åŸæ–‡ä¼ é€é—¨ï¼š

[MindSpore](https://www.mindspore.cn/tutorials/zh-CN/r2.2/beginner/train.html)

### å››ä¸ªæ­¥éª¤

1. æ„å»ºæ•°æ®é›†ã€‚
2. å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
3. å®šä¹‰è¶…å‚ã€æŸå¤±å‡½æ•°åŠä¼˜åŒ–å™¨ã€‚
4. è¾“å…¥æ•°æ®é›†è¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®é›†å’Œæ¨¡å‹åï¼Œå¯ä»¥è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒä¸è¯„ä¼°ã€‚

### æ„å»ºæ•°æ®é›†

ä»datasetåŠ è½½ä»£ç ï¼Œæ„å»ºæ•°æ®é›†

```
def datapipe(path, batch_size):
    image_transforms = [
        vision.Rescale(1.0 / 255.0, 0),
        vision.Normalize(mean=(0.1307,), std=(0.3081,)),
        vision.HWC2CHW()
    ]
    label_transform = transforms.TypeCast(mindspore.int32)

    dataset = MnistDataset(path)
    dataset = dataset.map(image_transforms, 'image')
    dataset = dataset.map(label_transform, 'label')
    dataset = dataset.batch(batch_size)
    return dataset

train_dataset = datapipe('MNIST_Data/train', batch_size=64)
test_dataset = datapipe('MNIST_Data/test', batch_size=64)
```

*class*mindspore.dataset.vision.HWC2CHW

å°†è¾“å…¥å›¾åƒçš„shapeä» <H, W, C> è½¬æ¢ä¸º <C, H, W>ã€‚ å¦‚æœè¾“å…¥å›¾åƒçš„shapeä¸º <H, W> ï¼Œå›¾åƒå°†ä¿æŒä¸å˜ã€‚

å¼‚å¸¸å¤„ç†

- **RuntimeError** - å¦‚æœè¾“å…¥å›¾åƒçš„shapeä¸æ˜¯ <H, W> æˆ– <H, W, C>ã€‚

*class*mindspore.nn.**ReLU**

é€å…ƒç´ æ±‚ max(0,ğ‘¥) ã€‚

- **x** (Tensor) - ç”¨äºè®¡ç®—**ReLU**çš„ä»»æ„ç»´åº¦çš„Tensorã€‚æ•°æ®ç±»å‹ä¸º [number](https://www.mindspore.cn/docs/zh-CN/r2.2/api_python/mindspore/mindspore.dtype.html#mindspore.dtype)ã€‚

### å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹

>  ä»[ç½‘ç»œæ„å»º](https://www.mindspore.cn/tutorials/zh-CN/r2.2/beginner/model.html)ä¸­åŠ è½½ä»£ç ï¼Œæ„å»ºä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

```python
class Network(nn.Cell):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.dense_relu_sequential = nn.SequentialCell(
            nn.Dense(28*28, 512),
            nn.ReLU(),
            nn.Dense(512, 512),
            nn.ReLU(),
            nn.Dense(512, 10)
        )

    def construct(self, x):
        x = self.flatten(x)
        logits = self.dense_relu_sequential(x)
        return logits

model = Network()
print(model)
```

### å®šä¹‰è¶…å‚ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

#### è¶…å‚

è¶…å‚ï¼ˆHyperparametersï¼‰æ˜¯å¯ä»¥è°ƒæ•´çš„å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶æ¨¡å‹è®­ç»ƒä¼˜åŒ–çš„è¿‡ç¨‹ï¼Œä¸åŒçš„è¶…å‚æ•°å€¼å¯èƒ½ä¼šå½±å“æ¨¡å‹è®­ç»ƒå’Œæ”¶æ•›é€Ÿåº¦ã€‚ç›®å‰æ·±åº¦å­¦ä¹ æ¨¡å‹å¤šé‡‡ç”¨æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•è¿›è¡Œä¼˜åŒ–ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„åŸç†å¦‚ä¸‹ï¼š
$$
w_{t+1}=w_{t}-\eta \frac{1}{n} \sum_{x \in \mathcal{B}} \nabla l\left(x, w_{t}\right)
$$
å…¬å¼ä¸­ï¼Œğ‘›æ˜¯æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰ï¼Œğœ‚æ˜¯å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ã€‚å¦å¤–ï¼Œğ‘¤ğ‘¡ä¸ºè®­ç»ƒè½®æ¬¡ğ‘¡ä¸­çš„æƒé‡å‚æ•°ï¼Œâˆ‡ğ‘™ä¸ºæŸå¤±å‡½æ•°çš„å¯¼æ•°ã€‚é™¤äº†æ¢¯åº¦æœ¬èº«ï¼Œè¿™ä¸¤ä¸ªå› å­ç›´æ¥å†³å®šäº†æ¨¡å‹çš„æƒé‡æ›´æ–°ï¼Œä»ä¼˜åŒ–æœ¬èº«æ¥çœ‹ï¼Œå®ƒä»¬æ˜¯å½±å“æ¨¡å‹æ€§èƒ½æ”¶æ•›æœ€é‡è¦çš„å‚æ•°ã€‚ä¸€èˆ¬ä¼šå®šä¹‰ä»¥ä¸‹è¶…å‚ç”¨äºè®­ç»ƒï¼š

- **è®­ç»ƒè½®æ¬¡ï¼ˆepochï¼‰**ï¼šè®­ç»ƒæ—¶éå†æ•°æ®é›†çš„æ¬¡æ•°ã€‚
- **æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰**ï¼šæ•°æ®é›†è¿›è¡Œåˆ†æ‰¹è¯»å–è®­ç»ƒï¼Œè®¾å®šæ¯ä¸ªæ‰¹æ¬¡æ•°æ®çš„å¤§å°ã€‚batch sizeè¿‡å°ï¼ŒèŠ±è´¹æ—¶é—´å¤šï¼ŒåŒæ—¶æ¢¯åº¦éœ‡è¡ä¸¥é‡ï¼Œä¸åˆ©äºæ”¶æ•›ï¼›batch sizeè¿‡å¤§ï¼Œä¸åŒbatchçš„æ¢¯åº¦æ–¹å‘æ²¡æœ‰ä»»ä½•å˜åŒ–ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æå°å€¼ï¼Œå› æ­¤éœ€è¦é€‰æ‹©åˆé€‚çš„batch sizeï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹ç²¾åº¦ã€å…¨å±€æ”¶æ•›ã€‚
- **å­¦ä¹ ç‡ï¼ˆlearning rateï¼‰**ï¼šå¦‚æœå­¦ä¹ ç‡åå°ï¼Œä¼šå¯¼è‡´æ”¶æ•›çš„é€Ÿåº¦å˜æ…¢ï¼Œå¦‚æœå­¦ä¹ ç‡åå¤§ï¼Œåˆ™å¯èƒ½ä¼šå¯¼è‡´è®­ç»ƒä¸æ”¶æ•›ç­‰ä¸å¯é¢„æµ‹çš„ç»“æœã€‚æ¢¯åº¦ä¸‹é™æ³•è¢«å¹¿æ³›åº”ç”¨åœ¨æœ€å°åŒ–æ¨¡å‹è¯¯å·®çš„å‚æ•°ä¼˜åŒ–ç®—æ³•ä¸Šã€‚æ¢¯åº¦ä¸‹é™æ³•é€šè¿‡å¤šæ¬¡è¿­ä»£ï¼Œå¹¶åœ¨æ¯ä¸€æ­¥ä¸­æœ€å°åŒ–æŸå¤±å‡½æ•°æ¥é¢„ä¼°æ¨¡å‹çš„å‚æ•°ã€‚å­¦ä¹ ç‡å°±æ˜¯åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œä¼šæ§åˆ¶æ¨¡å‹çš„å­¦ä¹ è¿›åº¦ã€‚

```python
epochs = 3
batch_size = 64
learning_rate = 1e-2
```

#### æŸå¤±å‡½æ•°

æŸå¤±å‡½æ•°ï¼ˆloss functionï¼‰ç”¨äºè¯„ä¼°æ¨¡å‹çš„é¢„æµ‹å€¼ï¼ˆlogitsï¼‰å’Œç›®æ ‡å€¼ï¼ˆtargetsï¼‰ä¹‹é—´çš„è¯¯å·®ã€‚è®­ç»ƒæ¨¡å‹æ—¶ï¼Œéšæœºåˆå§‹åŒ–çš„ç¥ç»ç½‘ç»œæ¨¡å‹å¼€å§‹æ—¶ä¼šé¢„æµ‹å‡ºé”™è¯¯çš„ç»“æœã€‚æŸå¤±å‡½æ•°ä¼šè¯„ä¼°é¢„æµ‹ç»“æœä¸ç›®æ ‡å€¼çš„ç›¸å¼‚ç¨‹åº¦ï¼Œæ¨¡å‹è®­ç»ƒçš„ç›®æ ‡å³ä¸ºé™ä½æŸå¤±å‡½æ•°æ±‚å¾—çš„è¯¯å·®ã€‚

å¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬ç”¨äºå›å½’ä»»åŠ¡çš„`nn.MSELoss`ï¼ˆå‡æ–¹è¯¯å·®ï¼‰å’Œç”¨äºåˆ†ç±»çš„`nn.NLLLoss`ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰ç­‰ã€‚ `nn.CrossEntropyLoss` ç»“åˆäº†`nn.LogSoftmax`å’Œ`nn.NLLLoss`ï¼Œå¯ä»¥å¯¹logits è¿›è¡Œå½’ä¸€åŒ–å¹¶è®¡ç®—é¢„æµ‹è¯¯å·®ã€‚

```python
loss_fn = nn.CrossEntropyLoss()
```

#### ä¼˜åŒ–å™¨

æ¨¡å‹ä¼˜åŒ–ï¼ˆOptimizationï¼‰æ˜¯åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­è°ƒæ•´æ¨¡å‹å‚æ•°ä»¥å‡å°‘æ¨¡å‹è¯¯å·®çš„è¿‡ç¨‹ã€‚MindSporeæä¾›å¤šç§ä¼˜åŒ–ç®—æ³•çš„å®ç°ï¼Œç§°ä¹‹ä¸ºä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰ã€‚ä¼˜åŒ–å™¨å†…éƒ¨å®šä¹‰äº†æ¨¡å‹çš„å‚æ•°ä¼˜åŒ–è¿‡ç¨‹ï¼ˆå³æ¢¯åº¦å¦‚ä½•æ›´æ–°è‡³æ¨¡å‹å‚æ•°ï¼‰ï¼Œæ‰€æœ‰ä¼˜åŒ–é€»è¾‘éƒ½å°è£…åœ¨ä¼˜åŒ–å™¨å¯¹è±¡ä¸­ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨SGDï¼ˆStochastic Gradient Descentï¼‰ä¼˜åŒ–å™¨ã€‚

æˆ‘ä»¬é€šè¿‡`model.trainable_params()`æ–¹æ³•è·å¾—æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ï¼Œå¹¶ä¼ å…¥å­¦ä¹ ç‡è¶…å‚æ¥åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚

```python
optimizer = nn.SGD(model.trainable_params(), learning_rate=learning_rate)
```

*class*mindspore.experimental.optim.**SGD**(*params*, *lr*, *momentum=0*, *dampening=0*, *weight_decay=0*, *nesterov=False*, ***, *maximize=False*)

éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚
$$
v_{t+1} = u \ast v_{t} + gradient \ast (1-dampening)
$$
å¦‚æœnesterovä¸ºTrueï¼š
$$
p_{t+1} = p_{t} - lr \ast (gradient + u \ast v_{t+1})
$$
å¦‚æœnesterovä¸ºFalseï¼š
$$
p_{t+1} = p_{t} - lr \ast v_{t+1}
$$
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºè®­ç»ƒçš„ç¬¬ä¸€æ­¥ ğ‘£ğ‘¡+1=ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ã€‚å…¶ä¸­ï¼Œpã€vå’Œuåˆ†åˆ«è¡¨ç¤º parametersã€accum å’Œ momentumã€‚

#### è®­ç»ƒä¸è¯„ä¼°

è®¾ç½®äº†è¶…å‚ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨åï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾ªç¯è¾“å…¥æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚ä¸€æ¬¡æ•°æ®é›†çš„å®Œæ•´è¿­ä»£å¾ªç¯ç§°ä¸ºä¸€è½®ï¼ˆepochï¼‰ã€‚æ¯è½®æ‰§è¡Œè®­ç»ƒæ—¶åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼š

1. è®­ç»ƒï¼šè¿­ä»£è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶å°è¯•æ”¶æ•›åˆ°æœ€ä½³å‚æ•°ã€‚
2. éªŒè¯/æµ‹è¯•ï¼šè¿­ä»£æµ‹è¯•æ•°æ®é›†ï¼Œä»¥æ£€æŸ¥æ¨¡å‹æ€§èƒ½æ˜¯å¦æå‡ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å®šä¹‰ç”¨äºè®­ç»ƒçš„`train_loop`å‡½æ•°å’Œç”¨äºæµ‹è¯•çš„`test_loop`å‡½æ•°ã€‚

ä½¿ç”¨å‡½æ•°å¼è‡ªåŠ¨å¾®åˆ†ï¼Œéœ€å…ˆå®šä¹‰æ­£å‘å‡½æ•°`forward_fn`ï¼Œä½¿ç”¨[value_and_grad](https://www.mindspore.cn/docs/zh-CN/r2.2/api_python/mindspore/mindspore.value_and_grad.html)è·å¾—å¾®åˆ†å‡½æ•°`grad_fn`ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†å¾®åˆ†å‡½æ•°å’Œä¼˜åŒ–å™¨çš„æ‰§è¡Œå°è£…ä¸º`train_step`å‡½æ•°ï¼Œæ¥ä¸‹æ¥å¾ªç¯è¿­ä»£æ•°æ®é›†è¿›è¡Œè®­ç»ƒå³å¯ã€‚

```python
# Define forward function
def forward_fn(data, label):
    logits = model(data)
    loss = loss_fn(logits, label)
    return loss, logits

# Get gradient function
grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)

# Define function of one-step training
def train_step(data, label):
    (loss, _), grads = grad_fn(data, label)
    optimizer(grads)
    return loss

def train_loop(model, dataset):
    size = dataset.get_dataset_size()
    model.set_train()
    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):
        loss = train_step(data, label)

        if batch % 100 == 0:
            loss, current = loss.asnumpy(), batch
            print(f"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]")
```

`test_loop`å‡½æ•°åŒæ ·éœ€å¾ªç¯éå†æ•°æ®é›†ï¼Œè°ƒç”¨æ¨¡å‹è®¡ç®—losså’ŒAccurayå¹¶è¿”å›æœ€ç»ˆç»“æœã€‚

```python
def test_loop(model, dataset, loss_fn):
    num_batches = dataset.get_dataset_size()
    model.set_train(False)
    total, test_loss, correct = 0, 0, 0
    for data, label in dataset.create_tuple_iterator():
        pred = model(data)
        total += len(data)
        test_loss += loss_fn(pred, label).asnumpy()
        correct += (pred.argmax(1) == label).asnumpy().sum()
    test_loss /= num_batches
    correct /= total
    print(f"Test: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
```

æˆ‘ä»¬å°†å®ä¾‹åŒ–çš„æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ä¼ å…¥`train_loop`å’Œ`test_loop`ä¸­ã€‚è®­ç»ƒ3è½®å¹¶è¾“å‡ºlosså’ŒAccuracyï¼ŒæŸ¥çœ‹æ€§èƒ½å˜åŒ–ã€‚

```python
loss_fn = nn.CrossEntropyLoss()
optimizer = nn.SGD(model.trainable_params(), learning_rate=learning_rate)

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train_loop(model, train_dataset)
    test_loop(model, test_dataset, loss_fn)
print("Done!")
```

> ```
> Epoch 1
> -------------------------------
> loss: 2.302806  [  0/938]
> loss: 2.285086  [100/938]
> loss: 2.264712  [200/938]
> loss: 2.174010  [300/938]
> loss: 1.931853  [400/938]
> loss: 1.340721  [500/938]
> loss: 0.953515  [600/938]
> loss: 0.756860  [700/938]
> loss: 0.756263  [800/938]
> loss: 0.463846  [900/938]
> Test:
>  Accuracy: 84.7%, Avg loss: 0.527155
> 
> Epoch 2
> -------------------------------
> loss: 0.479126  [  0/938]
> loss: 0.437443  [100/938]
> loss: 0.685504  [200/938]
> loss: 0.395121  [300/938]
> loss: 0.550566  [400/938]
> loss: 0.459457  [500/938]
> loss: 0.293049  [600/938]
> loss: 0.422102  [700/938]
> loss: 0.333153  [800/938]
> loss: 0.412182  [900/938]
> Test:
>  Accuracy: 90.5%, Avg loss: 0.335083
> 
> Epoch 3
> -------------------------------
> loss: 0.207366  [  0/938]
> loss: 0.343559  [100/938]
> loss: 0.391145  [200/938]
> loss: 0.317566  [300/938]
> loss: 0.200746  [400/938]
> loss: 0.445798  [500/938]
> loss: 0.603720  [600/938]
> loss: 0.170811  [700/938]
> loss: 0.411954  [800/938]
> loss: 0.315902  [900/938]
> Test:
>  Accuracy: 91.9%, Avg loss: 0.279034
> 
> Done!
> ```

è¯»è€…å¯ä»¥ä¿®æ”¹å‚æ•°å¤šè·‘å‡ è¾¹ï¼Œè§‚å¯Ÿæ•°æ®çš„å˜åŒ–ï¼Œä¸æ‡‚éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•æˆ–ä¸æ‡‚æ­£å‘åå‘ä¼ æ’­ã€å­¦ä¹ ç‡ä¹ƒè‡³è¶…å‚çš„æ¦‚å¿µå¯ä»¥å›é¡¾ä¹‹å‰æ–‡ç« ä¸­æåˆ°çš„è§†é¢‘å¹¶é‡å¤è§‚çœ‹ä¹‹å‰çš„æ–‡ç« ï¼Œè¿™å‡ ä¸ªæ¦‚å¿µåº”è¯¥æ¯”è¾ƒå¥½ç†è§£ã€‚